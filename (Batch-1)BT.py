# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11KQlTFNUK2QFwBrKR-talO2VsOC4nMMQ
"""

import numpy as np
import pandas as pd
import random
from typing import Dict, Tuple
import copy

DATASET_PATH = "/content/hfbt.csv"
try:
    df = pd.read_csv(DATASET_PATH)
except Exception as e:
    print("Load dataset: update DATASET_PATH to your uploaded CSV. Error:", e)
    df = pd.DataFrame()

print("Dataset rows:", len(df))
print(df.head())

def simulate_config(theta: Dict) -> Dict:
    """
    Fast approximate simulation that estimates:
    - energy_per_tx (J/tx)
    - throughput (tx/s)
    - latency (s)
    - fork_rate
    theta keys: block_size_kb, block_interval_s, validator_count, propagation_factor
    """
    # Unpack safely with default values
    bs = float(theta.get("block_size_kb", 1024.0))
    bi = float(theta.get("block_interval_s", 5.0))
    vc = int(theta.get("validator_count", 10))
    pf = float(theta.get("propagation_factor", 0.7))
    # Basic synthetic model assumptions:
    # tx per block = block_size_kb * 100 (assumed tx density)
    tx_per_block = bs * 100.0
    throughput = tx_per_block / bi  # tx/s
    # latency roughly half block interval + propagation delay
    base_prop = 0.05 + (bs / 10000.0)  # larger blocks -> higher propagation
    prop_delay = base_prop / pf
    latency = 0.5 * bi + prop_delay
    # fork probability increases with low block_interval and large block_size (simplified)
    fork_rate = max(0.0, 0.01 * (bs/1024.0) * (1.0 / max(1.0, bi/2.0)) * (1.0 + (50/vc)))
    # energy model: base energy per second per validator + per-block processing cost
    energy_per_sec_per_validator = 50.0  # J/s (toy number)
    block_processing_energy = 0.1 * bs  # J per KB of block
    blocks_per_sec = 1.0 / bi
    energy_total_per_sec = vc * energy_per_sec_per_validator + block_processing_energy * tx_per_block * blocks_per_sec
    energy_per_tx = energy_total_per_sec / max(throughput, 1e-6)
    return {
        "energy_per_tx": energy_per_tx,
        "throughput": throughput,
        "latency": latency,
        "fork_rate": fork_rate
    }

# Test simulate default
print(simulate_config({"block_size_kb":1024, "block_interval_s":5, "validator_count":10, "propagation_factor":0.7}))

T_MIN = 50.0
F_MAX = 0.02
L_MAX = 6.0
ALPHA = 500.0
BETA = 500.0
GAMMA = 500.0

def objective(theta: Dict) -> float:
    sim = simulate_config(theta)
    E = sim["energy_per_tx"]
    penalty = 0.0
    penalty += ALPHA * max(0.0, T_MIN - sim["throughput"])**2
    penalty += BETA * max(0.0, sim["fork_rate"] - F_MAX)**2
    penalty += GAMMA * max(0.0, sim["latency"] - L_MAX)**2
    return E + penalty

# -------------------------
# 4) Simple Genetic Algorithm
# -------------------------
PARAM_BOUNDS = {
    "block_size_kb": (64, 4096),
    "block_interval_s": (0.5, 20.0),
    "validator_count": (4, 100),
    "propagation_factor": (0.1, 1.0)
}

def random_individual():
    return {
        "block_size_kb": random.uniform(*PARAM_BOUNDS["block_size_kb"]),
        "block_interval_s": random.uniform(*PARAM_BOUNDS["block_interval_s"]),
        "validator_count": int(random.randint(*PARAM_BOUNDS["validator_count"])),
        "propagation_factor": random.uniform(*PARAM_BOUNDS["propagation_factor"])
    }

def mutate(ind, mut_rate=0.2):
    out = ind.copy()
    for k,(lo,hi) in PARAM_BOUNDS.items():
        if random.random() < mut_rate:
            if isinstance(lo, int) and isinstance(hi, int) and k=="validator_count":
                out[k] = int(np.clip(int(random.gauss(out[k], (hi-lo)*0.05)), lo, hi))
            else:
                out[k] = float(np.clip(random.gauss(out[k], (hi-lo)*0.05), lo, hi))
    return out

def crossover(a,b):
    child = {}
    for k in a.keys():
        child[k] = a[k] if random.random() < 0.5 else b[k]
    return child

def ga_optimize(pop_size=60, generations=120):
    pop = [random_individual() for _ in range(pop_size)]
    best_history = []
    for g in range(generations):
        scored = [(objective(ind), ind) for ind in pop]
        scored.sort(key=lambda x: x[0])
        best_history.append(scored[0])
        # print progress
        if g % 10 == 0:
            print(f"Gen {g:03d}: best obj {scored[0][0]:.4f}")
        # elitism + tournament + variation
        new_pop = [copy.deepcopy(scored[i][1]) for i in range(int(0.05*pop_size))]  # keep top 5%
        while len(new_pop) < pop_size:
            # tournament selection
            # a = min(random.sample(scored, 3))[1] # This was the problematic line
            # b = min(random.sample(scored, 3))[1] # This was the problematic line
            # Corrected lines:
            a = min(random.sample(scored, 3), key=lambda x: x[0])[1]
            b = min(random.sample(scored, 3), key=lambda x: x[0])[1]
            child = crossover(a,b)
            child = mutate(child, mut_rate=0.15)
            new_pop.append(child)
        pop = new_pop
    # return best
    scored = [(objective(ind), ind, simulate_config(ind)) for ind in pop]
    scored.sort(key=lambda x: x[0])
    return scored, best_history

results, history = ga_optimize(pop_size=40, generations=60)
print("Top result:", results[0][0], results[0][1], results[0][2])